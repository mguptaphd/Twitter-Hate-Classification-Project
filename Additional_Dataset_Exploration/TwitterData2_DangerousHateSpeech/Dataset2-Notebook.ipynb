{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn\n",
    "\n",
    "# NLTK/NLP\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "from nltk import FreqDist, word_tokenize\n",
    "import string, re\n",
    "import urllib\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "from nltk.collocations import *\n",
    "import gensim\n",
    "\n",
    "# Classifiers \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "#Sampling\n",
    "from imblearn.pipeline import make_pipeline, Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import sklearn.decomposition as decomposition\n",
    "\n",
    "#Visualization\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import customized functions\n",
    "# import import_ipynb\n",
    "# from custom_functions import *\n",
    "\n",
    "%run custom_functions_2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data2-cleaned.csv')\n",
    "df.drop(['Unnamed: 0'], axis = 1, inplace = True)\n",
    "df.lem_tweet= df.lem_tweet.apply(str)\n",
    "df.stem_tweet= df.stem_tweet.apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tidy_tweet</th>\n",
       "      <th>no_hash_tweet</th>\n",
       "      <th>tokenized_tweet</th>\n",
       "      <th>stemmed_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>lem_tweet</th>\n",
       "      <th>stem_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>woman shouldn complain about cleaning your hou...</td>\n",
       "      <td>woman shouldn complain about cleaning your hou...</td>\n",
       "      <td>['woman', 'shouldn', 'complain', 'about', 'cle...</td>\n",
       "      <td>['woman', 'shouldn', 'complain', 'about', 'cle...</td>\n",
       "      <td>['woman', 'shouldn', 'complain', 'about', 'cle...</td>\n",
       "      <td>woman shouldn complain about cleaning your hou...</td>\n",
       "      <td>woman shouldn complain about cleaning your hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>dats cold tyga cuffin place</td>\n",
       "      <td>dats cold tyga cuffin place</td>\n",
       "      <td>['dats', 'cold', 'tyga', 'cuffin', 'place']</td>\n",
       "      <td>['dat', 'cold', 'tyga', 'cuffin', 'place']</td>\n",
       "      <td>['dat', 'cold', 'tyga', 'cuffin', 'place']</td>\n",
       "      <td>dats cold tyga cuffin place</td>\n",
       "      <td>dats cold tyga cuffin plac</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>Dawg ever fuck bitch start confused shit</td>\n",
       "      <td>Dawg ever fuck bitch start confused shit</td>\n",
       "      <td>['Dawg', 'ever', 'fuck', 'bitch', 'start', 'co...</td>\n",
       "      <td>['dawg', 'ever', 'fuck', 'bitch', 'start', 'co...</td>\n",
       "      <td>['Dawg', 'ever', 'fuck', 'bitch', 'start', 'co...</td>\n",
       "      <td>Dawg ever fuck bitch start confused shit</td>\n",
       "      <td>dawg ever fuck bitch start confused shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>look like tranny</td>\n",
       "      <td>look like tranny</td>\n",
       "      <td>['look', 'like', 'tranny']</td>\n",
       "      <td>['look', 'like', 'tranni']</td>\n",
       "      <td>['look', 'like', 'tranny']</td>\n",
       "      <td>look like tranny</td>\n",
       "      <td>look like tranni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>shit hear about might true might faker than bi...</td>\n",
       "      <td>shit hear about might true might faker than bi...</td>\n",
       "      <td>['shit', 'hear', 'about', 'might', 'true', 'mi...</td>\n",
       "      <td>['shit', 'hear', 'about', 'might', 'true', 'mi...</td>\n",
       "      <td>['shit', 'hear', 'about', 'might', 'true', 'mi...</td>\n",
       "      <td>shit hear about might true might faker than bi...</td>\n",
       "      <td>shit hear about might true might faker than bi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...   \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   \n",
       "\n",
       "                                          tidy_tweet  \\\n",
       "0  woman shouldn complain about cleaning your hou...   \n",
       "1                        dats cold tyga cuffin place   \n",
       "2           Dawg ever fuck bitch start confused shit   \n",
       "3                                   look like tranny   \n",
       "4  shit hear about might true might faker than bi...   \n",
       "\n",
       "                                       no_hash_tweet  \\\n",
       "0  woman shouldn complain about cleaning your hou...   \n",
       "1                        dats cold tyga cuffin place   \n",
       "2           Dawg ever fuck bitch start confused shit   \n",
       "3                                   look like tranny   \n",
       "4  shit hear about might true might faker than bi...   \n",
       "\n",
       "                                     tokenized_tweet  \\\n",
       "0  ['woman', 'shouldn', 'complain', 'about', 'cle...   \n",
       "1        ['dats', 'cold', 'tyga', 'cuffin', 'place']   \n",
       "2  ['Dawg', 'ever', 'fuck', 'bitch', 'start', 'co...   \n",
       "3                         ['look', 'like', 'tranny']   \n",
       "4  ['shit', 'hear', 'about', 'might', 'true', 'mi...   \n",
       "\n",
       "                                      stemmed_tokens  \\\n",
       "0  ['woman', 'shouldn', 'complain', 'about', 'cle...   \n",
       "1         ['dat', 'cold', 'tyga', 'cuffin', 'place']   \n",
       "2  ['dawg', 'ever', 'fuck', 'bitch', 'start', 'co...   \n",
       "3                         ['look', 'like', 'tranni']   \n",
       "4  ['shit', 'hear', 'about', 'might', 'true', 'mi...   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  ['woman', 'shouldn', 'complain', 'about', 'cle...   \n",
       "1         ['dat', 'cold', 'tyga', 'cuffin', 'place']   \n",
       "2  ['Dawg', 'ever', 'fuck', 'bitch', 'start', 'co...   \n",
       "3                         ['look', 'like', 'tranny']   \n",
       "4  ['shit', 'hear', 'about', 'might', 'true', 'mi...   \n",
       "\n",
       "                                           lem_tweet  \\\n",
       "0  woman shouldn complain about cleaning your hou...   \n",
       "1                        dats cold tyga cuffin place   \n",
       "2           Dawg ever fuck bitch start confused shit   \n",
       "3                                   look like tranny   \n",
       "4  shit hear about might true might faker than bi...   \n",
       "\n",
       "                                          stem_tweet  \n",
       "0  woman shouldn complain about cleaning your hou...  \n",
       "1                         dats cold tyga cuffin plac  \n",
       "2           dawg ever fuck bitch start confused shit  \n",
       "3                                   look like tranni  \n",
       "4  shit hear about might true might faker than bi...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Test Split for TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['class'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting into train and test \n",
    "X_model, X_test, y_model, y_test = train_test_split(X, y, stratify = y,  test_size=0.20, random_state=123)\n",
    "\n",
    "#splitting \"model\" into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_model, y_model, test_size=0.20, random_state=123)\n",
    "\n",
    "# df_train_full = X_train.copy()\n",
    "# df_train_full['label']= y_train\n",
    "# train_full_df.to_csv('train_full_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.774321\n",
       "2    0.167978\n",
       "0    0.057701\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Vectorization and Method Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=.001)\n",
    "tfidf_ngram = TfidfVectorizer(ngram_range=(1,2), min_df=.001)\n",
    "tfidf_ngram2 = TfidfVectorizer(ngram_range=(2,3),min_df=.001)\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "rfc = RandomForestClassifier(random_state=10)\n",
    "nb = GaussianNB()\n",
    "svc = SVC(random_state=10)\n",
    "\n",
    "vectorization_list = [('COUNT_VECTORIZER', count_vect),\n",
    "                      ('TFIDF_VECTORIZER', tfidf_vectorizer),\n",
    "                      ('TFIDF_NGRAM_1_2', tfidf_ngram),\n",
    "                      ('TFIDF_NGRAM_2_3', tfidf_ngram2)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-12-9cade9a32372>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-9cade9a32372>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    X_val.lem_tweet, y_val, GaussianNB())\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# NB_compare_vectorization_model(X_train.lem_tweet, y_train, \n",
    "                                   X_val.lem_tweet, y_val, GaussianNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMOTE_vector_model(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, tfidf_vectorizer, logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions-2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_vector_model(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, tfidf_vectorizer, logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression: compare vectorizers with class weight balances + lemmatizing\n",
    "LR_cw_lemm = compare_vectorization_model(X_train.lem_tweet, \n",
    "                            y_train, X_val.lem_tweet, y_val, \n",
    "                            LogisticRegression(class_weight='balanced', solver = 'lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(LR_cw_lemm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression: compare vectorizers with SMOTE + lemmatizing\n",
    "LR_smote_lemm = SMOTE_compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, \n",
    "                                    y_val, LogisticRegression(class_weight='balanced', solver= 'lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_smote_lemm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression: compare vectorizers with upsampling + lemmatizing\n",
    "compare_vectorization_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                                   LogisticRegression(class_weight='balanced', solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression: compare vectorizers using stemming + class balances\n",
    "pd.DataFrame(compare_vectorization_model(X_train.stem_tweet, y_train, X_val.stem_tweet, \n",
    "                                    y_val, LogisticRegression(class_weight='balanced', solver='lbfgs')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Regularization:\n",
    "\n",
    "- Count Vectorizer:   \n",
    "\n",
    "l2 (default), no alpha tuning: F1: 0.99, 0.66\n",
    "C = .1:  .91,  .52\n",
    "C = .2:  .96,  .57\n",
    "C = .3:  .98,  .58\n",
    "C = .01:  .67,  .39\n",
    "C = .001:  .62, .39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_vector_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, count_vect, \n",
    "                   LogisticRegression(penalty = 'l1', C = .1,  class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class weight = balanced + lemmatized\n",
    "compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   SVC(class_weight ='balanced', gamma='auto', ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upsampling + lemmatized\n",
    "compare_vectorization_model(X_train_up.lem_tweet, y_train_up, X_val.lem_tweet, y_val, \n",
    "                                   SVC(class_weight ='balanced', gamma ='auto'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE + lemmatized \n",
    "SMOTE_compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, \n",
    "                                    y_val, SVC(class_weight ='balanced', gamma='auto', ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Searching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfid2 =  tfidf_ngram2.fit_transform(X_train_up.lemmatized_tweet)\n",
    "X_val_tfid2 =  tfidf_ngram2.transform(X_val.lemmatized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svc = SVC(kernel='linear', C=1, gamma=1, class_weight ='balanced')\n",
    "\n",
    "params = {\n",
    "'C': [0.1,.2, .3, 0.8,1,1.2,1.4],\n",
    "'kernel':['linear', 'rbf'],\n",
    "'gamma' :[0.1,0.8,1,1.2,1.4]\n",
    "}\n",
    "\n",
    "svm_gs= GridSearchCV(svc, param_grid = params, cv = 3)\n",
    "\n",
    "scores = ['f1','accuracy','recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs.fit(X_train_tfid2, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_vector_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vectorization_model(X_train_up.lemmatized_tweet, y_train_up, X_val.lemmatized_tweet, y_val, \n",
    "                                   SVC(C=1.2, cache_size=200, class_weight=None, coef0=0.0,\n",
    "    decision_function_shape='ovr', degree=3, gamma=1.4, kernel='rbf',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Multiple Methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with class weight balances + lemmatizing \n",
    "pd.DataFrame(compare_vectorization_model(X_train.lem_tweet, y_train, X_val.lem_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20, \n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with upsampling + lemmatizing \n",
    "compare_vectorization_model(X_train_up.lemmatized_tweet, y_train_up, X_val.lemmatized_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20,\n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with SMOTE + lemmatizing  \n",
    "SMOTE_compare_vectorization_model(X_train.lemmatized_tweet, y_train, X_val.lemmatized_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20,\n",
    "                                   n_estimators = 100, class_weight = 'balanced', random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with upsampling + stemming\n",
    "compare_vectorization_model(X_train.stemmed_tweet_meta, y_train, X_val.stemmed_tweet_meta, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 20,\n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Fine-Tuning Hyperparameters: Max depth 10.... regularization??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest: compare vectorizers with upsampling + lemmatizing \n",
    "compare_vectorization_model(X_train_up.lemmatized_tweet, y_train_up, X_val.lemmatized_tweet, y_val, \n",
    "                                   RandomForestClassifier(max_depth= 10,\n",
    "                                   n_estimators = 100, class_weight='balanced', random_state=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vectorization_model(X_train_up.lemmatized_tweet, y_train_up, X_val.lemmatized_tweet, y_val, \n",
    "                                   RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "                       criterion='gini', max_depth=20, max_features='auto',\n",
    "                       max_leaf_nodes=200, min_impurity_decrease=0.0,\n",
    "                       min_impurity_split=None, min_samples_leaf=1,\n",
    "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
    "                       random_state=10, verbose=0, warm_start=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid-Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "# rfc = RandomForestClassifier(n_estimators=60, max_depth=6, random_state=10, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "parameters = {'n_estimators' : [40, 60, 80, 100],\n",
    "'max_leaf_nodes' : [200, 400, 600],\n",
    "'random_state' : [10],\n",
    "'max_depth': [5, 7, 10, 20],\n",
    " 'verbose' : [0],\n",
    "'class_weight': ['balanced']\n",
    "             }\n",
    "          \n",
    "rfc_gs = GridSearchCV(\n",
    "    RandomForestClassifier(class_weight='balanced', random_state = 10), param_grid=parameters, cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.fit(X_train_countvect, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc_gs.score(X_val_countvect, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt with New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countvect =  count_vect.fit_transform(X_train.lem_tweet)\n",
    "X_val_countvect =  count_vect.transform(X_val.lem_tweet)\n",
    "# X_test_countvect = count_vect.transform(X_test.lemmatized_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc2 = RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
    "                       criterion='gini', max_depth=20, max_features='auto',\n",
    "                       max_leaf_nodes=200, min_impurity_decrease=0.0,\n",
    "                       min_impurity_split=None, min_samples_leaf=1,\n",
    "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
    "                       random_state=10, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "                       criterion='gini', max_depth=20, max_features='auto',\n",
       "                       max_leaf_nodes=200, min_impurity_decrease=0.0,\n",
       "                       min_impurity_split=None, min_samples_leaf=1,\n",
       "                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "                       n_estimators=100, n_jobs=None, oob_score=False,\n",
       "                       random_state=10, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc2.fit (X_train_countvect, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8404161412358133"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_predict = rfc2.predict(X_train_countvect)\n",
    "metrics.accuracy_score(y_train, y_train_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7382561705291922"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_train, y_train_predict, average ='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15860,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = rfc2.predict(X_val_countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_prob = rfc2.predict_proba(X_val_countvect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108</td>\n",
       "      <td>44</td>\n",
       "      <td>47</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>215</td>\n",
       "      <td>2430</td>\n",
       "      <td>433</td>\n",
       "      <td>3078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>665</td>\n",
       "      <td>689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>335</td>\n",
       "      <td>2486</td>\n",
       "      <td>1145</td>\n",
       "      <td>3966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0     1     2   All\n",
       "Actual                          \n",
       "0          108    44    47   199\n",
       "1          215  2430   433  3078\n",
       "2           12    12   665   689\n",
       "All        335  2486  1145  3966"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_test = pd.crosstab(y_val, y_val_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "\n",
    "confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6677191812625272"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.f1_score(y_val, y_val_pred,  average ='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.626882646494138"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_score(y_val, y_val_pred,  average ='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7657847202042857"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.recall_score(y_val, y_val_pred, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.774321\n",
       "2    0.167978\n",
       "0    0.057701\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([16608,  8132,  8290,  5787,  1929, 13835,  8311, 20603, 13164,\n",
       "             1410,\n",
       "            ...\n",
       "             7277, 19527, 22551,  8053,  8964,   996,  1844,  6148, 11603,\n",
       "            18078],\n",
       "           dtype='int64', length=3966)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>predicted_class</th>\n",
       "      <th>actual_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16608</th>\n",
       "      <td>0.302875</td>\n",
       "      <td>0.305064</td>\n",
       "      <td>0.392061</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8132</th>\n",
       "      <td>0.299460</td>\n",
       "      <td>0.329408</td>\n",
       "      <td>0.371131</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8290</th>\n",
       "      <td>0.293152</td>\n",
       "      <td>0.307668</td>\n",
       "      <td>0.399180</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>0.320898</td>\n",
       "      <td>0.405600</td>\n",
       "      <td>0.273502</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>0.281834</td>\n",
       "      <td>0.453192</td>\n",
       "      <td>0.264974</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13835</th>\n",
       "      <td>0.346753</td>\n",
       "      <td>0.281292</td>\n",
       "      <td>0.371955</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8311</th>\n",
       "      <td>0.350086</td>\n",
       "      <td>0.442718</td>\n",
       "      <td>0.207196</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20603</th>\n",
       "      <td>0.302881</td>\n",
       "      <td>0.421474</td>\n",
       "      <td>0.275645</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13164</th>\n",
       "      <td>0.310287</td>\n",
       "      <td>0.423279</td>\n",
       "      <td>0.266434</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1410</th>\n",
       "      <td>0.298539</td>\n",
       "      <td>0.390028</td>\n",
       "      <td>0.311433</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22263</th>\n",
       "      <td>0.305381</td>\n",
       "      <td>0.399411</td>\n",
       "      <td>0.295208</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13646</th>\n",
       "      <td>0.295946</td>\n",
       "      <td>0.326717</td>\n",
       "      <td>0.377337</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17203</th>\n",
       "      <td>0.272561</td>\n",
       "      <td>0.296172</td>\n",
       "      <td>0.431268</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1103</th>\n",
       "      <td>0.300894</td>\n",
       "      <td>0.393833</td>\n",
       "      <td>0.305273</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11246</th>\n",
       "      <td>0.358985</td>\n",
       "      <td>0.331505</td>\n",
       "      <td>0.309510</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4912</th>\n",
       "      <td>0.320934</td>\n",
       "      <td>0.337035</td>\n",
       "      <td>0.342032</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16648</th>\n",
       "      <td>0.279674</td>\n",
       "      <td>0.289516</td>\n",
       "      <td>0.430810</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>0.262781</td>\n",
       "      <td>0.257924</td>\n",
       "      <td>0.479295</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8164</th>\n",
       "      <td>0.309682</td>\n",
       "      <td>0.403218</td>\n",
       "      <td>0.287100</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21629</th>\n",
       "      <td>0.318444</td>\n",
       "      <td>0.433126</td>\n",
       "      <td>0.248430</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11459</th>\n",
       "      <td>0.302325</td>\n",
       "      <td>0.389727</td>\n",
       "      <td>0.307948</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19338</th>\n",
       "      <td>0.322278</td>\n",
       "      <td>0.327370</td>\n",
       "      <td>0.350352</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6761</th>\n",
       "      <td>0.296881</td>\n",
       "      <td>0.415456</td>\n",
       "      <td>0.287663</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18580</th>\n",
       "      <td>0.291364</td>\n",
       "      <td>0.436062</td>\n",
       "      <td>0.272575</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19688</th>\n",
       "      <td>0.295561</td>\n",
       "      <td>0.427869</td>\n",
       "      <td>0.276571</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11721</th>\n",
       "      <td>0.305527</td>\n",
       "      <td>0.420968</td>\n",
       "      <td>0.273505</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1734</th>\n",
       "      <td>0.292823</td>\n",
       "      <td>0.425468</td>\n",
       "      <td>0.281710</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17459</th>\n",
       "      <td>0.363124</td>\n",
       "      <td>0.339869</td>\n",
       "      <td>0.297007</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21136</th>\n",
       "      <td>0.311894</td>\n",
       "      <td>0.295920</td>\n",
       "      <td>0.392186</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24682</th>\n",
       "      <td>0.310582</td>\n",
       "      <td>0.376415</td>\n",
       "      <td>0.313003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13872</th>\n",
       "      <td>0.337917</td>\n",
       "      <td>0.323809</td>\n",
       "      <td>0.338274</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10930</th>\n",
       "      <td>0.298671</td>\n",
       "      <td>0.429211</td>\n",
       "      <td>0.272118</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18554</th>\n",
       "      <td>0.338389</td>\n",
       "      <td>0.302360</td>\n",
       "      <td>0.359251</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21596</th>\n",
       "      <td>0.327108</td>\n",
       "      <td>0.321552</td>\n",
       "      <td>0.351339</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22138</th>\n",
       "      <td>0.305881</td>\n",
       "      <td>0.410570</td>\n",
       "      <td>0.283549</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14374</th>\n",
       "      <td>0.280999</td>\n",
       "      <td>0.274877</td>\n",
       "      <td>0.444125</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24238</th>\n",
       "      <td>0.340707</td>\n",
       "      <td>0.423155</td>\n",
       "      <td>0.236138</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17781</th>\n",
       "      <td>0.293264</td>\n",
       "      <td>0.436329</td>\n",
       "      <td>0.270407</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13921</th>\n",
       "      <td>0.303012</td>\n",
       "      <td>0.424010</td>\n",
       "      <td>0.272978</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15055</th>\n",
       "      <td>0.308639</td>\n",
       "      <td>0.303404</td>\n",
       "      <td>0.387957</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24302</th>\n",
       "      <td>0.329491</td>\n",
       "      <td>0.325311</td>\n",
       "      <td>0.345198</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6365</th>\n",
       "      <td>0.297044</td>\n",
       "      <td>0.426305</td>\n",
       "      <td>0.276650</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24671</th>\n",
       "      <td>0.326344</td>\n",
       "      <td>0.429966</td>\n",
       "      <td>0.243690</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7972</th>\n",
       "      <td>0.301912</td>\n",
       "      <td>0.424652</td>\n",
       "      <td>0.273436</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7971</th>\n",
       "      <td>0.341704</td>\n",
       "      <td>0.391171</td>\n",
       "      <td>0.267125</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13317</th>\n",
       "      <td>0.322130</td>\n",
       "      <td>0.470917</td>\n",
       "      <td>0.206953</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22404</th>\n",
       "      <td>0.371982</td>\n",
       "      <td>0.332662</td>\n",
       "      <td>0.295356</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6780</th>\n",
       "      <td>0.311304</td>\n",
       "      <td>0.423408</td>\n",
       "      <td>0.265288</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19238</th>\n",
       "      <td>0.302019</td>\n",
       "      <td>0.416261</td>\n",
       "      <td>0.281720</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22597</th>\n",
       "      <td>0.307137</td>\n",
       "      <td>0.314470</td>\n",
       "      <td>0.378393</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7277</th>\n",
       "      <td>0.297044</td>\n",
       "      <td>0.426305</td>\n",
       "      <td>0.276650</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19527</th>\n",
       "      <td>0.332593</td>\n",
       "      <td>0.494739</td>\n",
       "      <td>0.172669</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22551</th>\n",
       "      <td>0.343495</td>\n",
       "      <td>0.272352</td>\n",
       "      <td>0.384153</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8053</th>\n",
       "      <td>0.300688</td>\n",
       "      <td>0.430142</td>\n",
       "      <td>0.269171</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8964</th>\n",
       "      <td>0.318528</td>\n",
       "      <td>0.320615</td>\n",
       "      <td>0.360857</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.299336</td>\n",
       "      <td>0.426196</td>\n",
       "      <td>0.274468</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1844</th>\n",
       "      <td>0.325295</td>\n",
       "      <td>0.387381</td>\n",
       "      <td>0.287324</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6148</th>\n",
       "      <td>0.301281</td>\n",
       "      <td>0.424208</td>\n",
       "      <td>0.274512</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11603</th>\n",
       "      <td>0.394029</td>\n",
       "      <td>0.297573</td>\n",
       "      <td>0.308398</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18078</th>\n",
       "      <td>0.312893</td>\n",
       "      <td>0.415737</td>\n",
       "      <td>0.271371</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3966 rows  5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2  predicted_class  actual_class\n",
       "16608  0.302875  0.305064  0.392061                2             2\n",
       "8132   0.299460  0.329408  0.371131                2             1\n",
       "8290   0.293152  0.307668  0.399180                2             1\n",
       "5787   0.320898  0.405600  0.273502                1             1\n",
       "1929   0.281834  0.453192  0.264974                1             1\n",
       "13835  0.346753  0.281292  0.371955                2             2\n",
       "8311   0.350086  0.442718  0.207196                1             1\n",
       "20603  0.302881  0.421474  0.275645                1             1\n",
       "13164  0.310287  0.423279  0.266434                1             1\n",
       "1410   0.298539  0.390028  0.311433                1             1\n",
       "22263  0.305381  0.399411  0.295208                1             1\n",
       "13646  0.295946  0.326717  0.377337                2             2\n",
       "17203  0.272561  0.296172  0.431268                2             2\n",
       "1103   0.300894  0.393833  0.305273                1             1\n",
       "11246  0.358985  0.331505  0.309510                0             1\n",
       "4912   0.320934  0.337035  0.342032                2             1\n",
       "16648  0.279674  0.289516  0.430810                2             2\n",
       "1965   0.262781  0.257924  0.479295                2             2\n",
       "8164   0.309682  0.403218  0.287100                1             1\n",
       "21629  0.318444  0.433126  0.248430                1             1\n",
       "11459  0.302325  0.389727  0.307948                1             1\n",
       "19338  0.322278  0.327370  0.350352                2             1\n",
       "6761   0.296881  0.415456  0.287663                1             1\n",
       "18580  0.291364  0.436062  0.272575                1             1\n",
       "19688  0.295561  0.427869  0.276571                1             1\n",
       "11721  0.305527  0.420968  0.273505                1             1\n",
       "1734   0.292823  0.425468  0.281710                1             1\n",
       "17459  0.363124  0.339869  0.297007                0             1\n",
       "21136  0.311894  0.295920  0.392186                2             2\n",
       "24682  0.310582  0.376415  0.313003                1             1\n",
       "...         ...       ...       ...              ...           ...\n",
       "13872  0.337917  0.323809  0.338274                2             1\n",
       "10930  0.298671  0.429211  0.272118                1             1\n",
       "18554  0.338389  0.302360  0.359251                2             1\n",
       "21596  0.327108  0.321552  0.351339                2             1\n",
       "22138  0.305881  0.410570  0.283549                1             1\n",
       "14374  0.280999  0.274877  0.444125                2             2\n",
       "24238  0.340707  0.423155  0.236138                1             1\n",
       "17781  0.293264  0.436329  0.270407                1             1\n",
       "13921  0.303012  0.424010  0.272978                1             1\n",
       "15055  0.308639  0.303404  0.387957                2             0\n",
       "24302  0.329491  0.325311  0.345198                2             2\n",
       "6365   0.297044  0.426305  0.276650                1             1\n",
       "24671  0.326344  0.429966  0.243690                1             1\n",
       "7972   0.301912  0.424652  0.273436                1             1\n",
       "7971   0.341704  0.391171  0.267125                1             1\n",
       "13317  0.322130  0.470917  0.206953                1             1\n",
       "22404  0.371982  0.332662  0.295356                0             1\n",
       "6780   0.311304  0.423408  0.265288                1             1\n",
       "19238  0.302019  0.416261  0.281720                1             1\n",
       "22597  0.307137  0.314470  0.378393                2             2\n",
       "7277   0.297044  0.426305  0.276650                1             1\n",
       "19527  0.332593  0.494739  0.172669                1             0\n",
       "22551  0.343495  0.272352  0.384153                2             2\n",
       "8053   0.300688  0.430142  0.269171                1             1\n",
       "8964   0.318528  0.320615  0.360857                2             1\n",
       "996    0.299336  0.426196  0.274468                1             1\n",
       "1844   0.325295  0.387381  0.287324                1             1\n",
       "6148   0.301281  0.424208  0.274512                1             1\n",
       "11603  0.394029  0.297573  0.308398                0             1\n",
       "18078  0.312893  0.415737  0.271371                1             1\n",
       "\n",
       "[3966 rows x 5 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame(y_val_prob)\n",
    "pred_df.index = y_val.index \n",
    "pred_df['predicted_class'] = y_val_pred\n",
    "pred_df['actual_class'] = y_val\n",
    "pred_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>predicted_class</th>\n",
       "      <th>actual_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16608</th>\n",
       "      <td>0.302875</td>\n",
       "      <td>0.305064</td>\n",
       "      <td>0.392061</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8132</th>\n",
       "      <td>0.299460</td>\n",
       "      <td>0.329408</td>\n",
       "      <td>0.371131</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8290</th>\n",
       "      <td>0.293152</td>\n",
       "      <td>0.307668</td>\n",
       "      <td>0.399180</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5787</th>\n",
       "      <td>0.320898</td>\n",
       "      <td>0.405600</td>\n",
       "      <td>0.273502</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1929</th>\n",
       "      <td>0.281834</td>\n",
       "      <td>0.453192</td>\n",
       "      <td>0.264974</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2  predicted_class  actual_class\n",
       "16608  0.302875  0.305064  0.392061                2             2\n",
       "8132   0.299460  0.329408  0.371131                2             1\n",
       "8290   0.293152  0.307668  0.399180                2             1\n",
       "5787   0.320898  0.405600  0.273502                1             1\n",
       "1929   0.281834  0.453192  0.264974                1             1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df[pred_df['predicted_class'] != pred_df['actual_class']]\n",
    "pred_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'THIS honor Native Americans #changethename #HTTR http VGQvEHy'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tidy_tweet[16608]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "sentence = 'people suck'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(sentence):\n",
    "    score = analyzer.polarity_scores(sentence)['compound']\n",
    "    print(\"{:-<40} {}\".format(sentence, str(score)))\n",
    "\n",
    "    if compound > 0:\n",
    "        return 1  ## positive\n",
    "    else:\n",
    "        return 0 ## negative\n",
    "   # else:\n",
    "        #return \"Neutral\"     \n",
    "    return compound\n",
    "\n",
    "sentiment_analyzer_scores(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_val, y_val_pred, margins = True)\n",
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_vectorization_model(X_train.lemmatized_tweet, y_train, X_val.lemmatized_tweet, y_val, \n",
    "                     RandomForestClassifier(class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word2vec = gensim.models.Word2Vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t = time()\n",
    "\n",
    "# word2vec.build_vocab(df_tokenized_list, progress_per=10000)\n",
    "\n",
    "# print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word to Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X-train pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.tokenized_tweet.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train.tokenized_tweet.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_list = list(X_train['tokenized_tweet'])\n",
    "X_train_token_sumlist = sum(X_train_token_list,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unique_tokens = set(X_train_token_sumlist)\n",
    "print('The unique number of words in the training dataset is: {}'.format(len(X_train_unique_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_unique_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X-val pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_val_token_list = list(X_val['tokenized_tweet'])\n",
    "# X_val_token_sumlist = sum(X_val_token_list,[])\n",
    "# X_val_unique_tokens = set(X_val_token_sumlist)\n",
    "\n",
    "# print('The unique number of words in the validation dataset is: {}'.format(len(X_val_unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### X-test pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_token_list = list(X_test['tokenized_tweet'])\n",
    "# X_test_token_sumlist = sum(X_test_token_list,[])\n",
    "\n",
    "# X_test_unique_tokens = set(X_test_token_sumlist)\n",
    "# print('The unique number of words in the training dataset is: {}'.format(len(X_test_unique_tokens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "t = time()\n",
    "\n",
    "w2v = gensim.models.Word2Vec(X_train_token_list, sg=1, min_count=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.train(X_train_token_list, total_examples=w2v.corpus_count, epochs=w2v.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2v.save('w2v-min1.model')\n",
    "# w2v = gensim.models.Word2Vec.load('w2v-min1.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "w2v.corpus_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vocab= w2v.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(w2v_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv['trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(['trump'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.most_similar(positive=['lazy','black'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv.get_keras_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_X = w2v.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = df_tokenized_list[1]\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([w2v[w] for w in sentence if w in w2v]\n",
    "                   or [np.zeros(100)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " np.mean([w2v[w] for w in sentence if w in w2v]  or np.zeros(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr = np.empty((31410, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(input_to_lr, np.mean([w2v[w] for w in sentence if w in w2v]\n",
    "                   or [np.zeros(100)], axis=0))\n",
    "# np.mean([w2v[w] for w in sentence if w in w2v], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp = input_to_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_temp_df = pd.DataFrame(X_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.fit(X_train_temp, y)\n",
    "a.score(X_train_temp, y)\n",
    "c = a.predict(X_train_temp)\n",
    "# print scores  \n",
    "print('Train Accuracy: ' + str(round(metrics.f1_score(y, c),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample, X_train_remainder, y_train_sample, y_train_remainder = train_test_split(X_train, y_train, test_size=0.99, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_RNN_sample= X_train_sample['tokenized_tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_RNN_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_RNN_sample=y_train_sample\n",
    "y_RNN_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = X_RNN_sample\n",
    "# define class labels\n",
    "labels = y_RNN_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "vocab_size = 100\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "print(encoded_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = gensim.models.Word2Vec(df_tokenized_list, size=dimsize, window=5, min_count=50, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsne_plot(w2v_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run custom_functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#size of hidden layer (length of continuous word representation)\n",
    "dimsize= 100\n",
    "\n",
    "# model_w2v = gensim.models.Word2Vec(X_train_token_list, size= dimsize, window=5, min_count=1, workers=4)\n",
    "model_w2v = gensim.models.Word2Vec(X_train_token_list, size= dimsize,min_count=1)\n",
    "\n",
    "\n",
    "\n",
    "#create average vector for train and test from model\n",
    "#returned list of numpy arrays are then stacked \n",
    "X_train_w2v = np.concatenate([avg_word_vectors(w, dimsize, model_w2v) for w in X_train_token_list])\n",
    "X_val_w2v = np.concatenate([avg_word_vectors(w,dimsize, model_w2v) for w in X_val_token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val_w2v[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_token_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr = np.empty((31410, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(input_to_lr, np.mean([model_w2v[w] for w in sentence if w in model_w2v]\n",
    "                   or [np.zeros(100)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_w2v_model (X_train_w2v, y_train, X_val_w2v, y_val, classifier):\n",
    "        \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "        \n",
    "    pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_w2v, y_train)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_w2v)\n",
    "    val_predictions = model.predict (X_val_w2v)\n",
    "    \n",
    "   # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    log_confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    return log_confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, RandomForestClassifier(n_estimators=100, max_depth= 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_smote_w2v_model (X_train_w2v, y_train, X_val_w2v, y_val, classifier):\n",
    "    \n",
    "    pca = decomposition.PCA(n_components=50)\n",
    "    \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "        \n",
    "    pipe = make_pipeline(pca, smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_w2v, y_train)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_w2v)\n",
    "    val_predictions = model.predict (X_val_w2v)\n",
    "    \n",
    "   # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    log_confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    return log_confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, LogisticRegression(solver='lbfgs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_smote_w2v_model(X_train_w2v, y_train, X_val_w2v, y_val, RandomForestClassifier(n_estimators=100, max_depth=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install glove_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install glovepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "filename = 'glove.twitter.27B.100d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_input_file = 'glove.twitter.27B.100d.txt'\n",
    "glove_output_file = 'glove.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, glove_output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model = KeyedVectors.load_word2vec_format('glove.txt.word2vec', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_model['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(glove_model.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove = np.concatenate([avg_word_vectors(w, dimsize, glove_model) for w in X_train_token_list])\n",
    "X_val_glove = np.concatenate([avg_word_vectors(w, dimsize, glove_model) for w in X_val_token_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove[255]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_2 = np.empty((31410, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(input_to_lr, np.mean([glove_model[w] for w in sentence if w in glove_model]\n",
    "                   or [np.zeros(100)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove_2[225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Learnco "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict = {}\n",
    "with open('glove.twitter.27B.100d.txt', 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in X_train_unique_tokens:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove_dict[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_dict['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input_to_lr = np.empty((31410, 100))\n",
    "for sentence in X_train_token_list:\n",
    "    np.append(input_to_lr, np.mean([w2v[w] for w in sentence if w in w2v]\n",
    "                   or [np.zeros(100)], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_to_lr['love']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_smote_w2v_model(X_train_glove, y_train, X_val_glove, y_val, RandomForestClassifier(n_estimators=100, max_depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smote_w2v_model (X_train_w2v, y_train, X_val_w2v, y_val, classifier):\n",
    "        \n",
    "    smote = SMOTE(random_state=1, sampling_strategy='not majority')\n",
    "        \n",
    "    pipe = make_pipeline(smote, classifier) \n",
    "    \n",
    "    model = pipe.fit(X_train_w2v, y_train)\n",
    "    \n",
    "    train_predictions = model.predict(X_train_w2v)\n",
    "    val_predictions = model.predict (X_val_w2v)\n",
    "    \n",
    "   # print scores  \n",
    "    print('Train Accuracy: ' + str(round(metrics.accuracy_score(y_train, train_predictions),2)))\n",
    "    print('Train Precision: ' + str(round(metrics.precision_score(y_train, train_predictions),2)))\n",
    "    print('Train Recall: ' + str(round(metrics.recall_score(y_train, train_predictions),2)))\n",
    "    print('Train F1: ' + str(round(metrics.f1_score(y_train, train_predictions),2)))\n",
    "    print('\\n')\n",
    "    print('Validation Accuracy: ' + str(round(metrics.accuracy_score(y_val, val_predictions),2)))\n",
    "    print('Validation Precision: ' + str(round(metrics.precision_score(y_val, val_predictions),2)))\n",
    "    print('Validation Recall: ' + str(round(metrics.recall_score(y_val, val_predictions),2)))\n",
    "    print('Validation F1: ' + str(round(metrics.f1_score(y_val, val_predictions),2)))\n",
    "\n",
    "    log_confusion_test = pd.crosstab(y_val, val_predictions, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "    \n",
    "    return log_confusion_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, RandomForestClassifier(max_depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, RandomForestClassifier(max_depth=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, LogisticRegression(penalty ='l1', C = 10,\n",
    "                                                                            class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, LogisticRegression(penalty ='l1', C = .001,\n",
    "                                                                            class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, LogisticRegression(penalty ='l1', class_weight='balanced'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, \n",
    "                 LogisticRegression(penalty ='l2', C = 5, class_weight ={0: 5 , 1: 5}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_w2v_model (X_train_glove, y_train, X_val_glove, y_val, \n",
    "                 LogisticRegression(penalty ='l2', C = .1, class_weight ={0: 5 , 1: 5}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Trump Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trump_df= pd.read_csv('data/cleaned-trump-tweet.csv')\n",
    "trump_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_df.stem_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_countvect =  count_vect.fit_transform(X_train_up.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X_train_countvect, y_train_up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trump = count_vect.transform(trump_df.lem_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trump = X_trump.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trump.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_up.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
